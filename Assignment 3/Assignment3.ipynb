{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, IntegerType}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.mllib.random.RandomRDDs._\n",
    "import org.apache.spark.rdd\n",
    "import org.apache.spark.sql.DataFrameReader\n",
    "import org.apache.spark.mllib.rdd.RDDFunctions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Write a function that will put N doubles into a file. The doubles need to be normally distributed with mean 0 and standard deviation 1. The function should have two arguments: N and the full name of the file (ie includes path to file location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nDoublesNormalDistribution(N:Int, path: String) : Unit = {\n",
    "    val result = normalRDD(sc, N)\n",
    "    result.saveAsTextFile(path)\n",
    "}\n",
    "\n",
    "nDoublesNormalDistribution(100,\"output1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a file with 50,000 doubles using the function from problem 1. This file will be used for the next several problems. It is best if you put the file in the current directory to avoid paths that do not exist on other machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nDoublesNormalDistribution(N:Int, path: String) : Unit = {\n",
    "    val result = normalRDD(sc, N)\n",
    "    result.saveAsTextFile(path)\n",
    "}\n",
    "\n",
    "nDoublesNormalDistribution(50000,\"output.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Read the file created in #2 into an RDD and compute the mean and standard deviation of the doubles in the file. Work on the RDD, that is do not convert the RDD to a DataFrame or Dataset.. You are to use Spark code to compute the values as we want this to run on a cluster using multiple machines. So the pure Scala code you used in assignment will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean :-0.0010871448803535851\n",
      "Standard Deviation :1.0006187579378962"
     ]
    }
   ],
   "source": [
    "val outputFileData = sc.textFile(\"output.txt\").map(x => x.toDouble)\n",
    "println(\"Mean :\" + outputFileData.mean)\n",
    "print(\"Standard Deviation :\" + outputFileData.stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3) Repeat #3 but using a DataFrame instead of RDD. Here work on the DataFrame not an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                Mean|Standard Deviation|\n",
      "+--------------------+------------------+\n",
      "|-0.00108714488035...|1.0006287642755711|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val dataFrame = spark.read.text(\"output.txt\")\n",
    "val meanAndStandradDeviation = dataFrame.selectExpr(\"mean(value) as `Mean`\",\"stddev(value) as `Standard Deviation`\")\n",
    "meanAndStandradDeviation.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Using a DataFrame create a random sample of about 100 elements of the file created in #2 and compute the mean of the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|        Sample Mean|Number of Elements|\n",
      "+-------------------+------------------+\n",
      "|0.02866426042080432|               100|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val dataFrameSample = spark.read.text(\"output.txt\")\n",
    "val sampledData = dataFrameSample.sample(false,0.003)\n",
    "val sampledDataHundered = sampledData.limit(100)\n",
    "val meanSampledData = sampledDataHundered.selectExpr(\"mean(value) as `Sample Mean`\",\"count(value) as `Number of Elements`\")\n",
    "meanSampledData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Create a file of 100 normally distributed doubles. Read the doubles from the file into an RDD. Using the RDD create a sliding window of size 20 and compute the mean of each window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hunderedDoublesNormalDistribution(N:Int, path: String) : Unit = {\n",
    "    val result = normalRDD(sc, N)\n",
    "    result.saveAsTextFile(path)\n",
    "}\n",
    "\n",
    "hunderedDoublesNormalDistribution(100,\"HunderedDoubles.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The means of the windows of size 20: \n",
      "0.1835441334650994\n",
      "0.28727539528616813\n",
      "0.19979783925901715\n",
      "0.20210602542027423\n",
      "0.11973748267104237\n",
      "-0.0381558335979186\n",
      "0.015940549390319286\n",
      "0.06056421060304512\n",
      "0.05857339655049243\n",
      "0.0421041007779791\n",
      "0.04124518473699818\n",
      "-0.07188843413672932\n",
      "-0.08217699652981544\n",
      "-0.03313465064788966\n",
      "-0.038366522417607206\n",
      "0.04147658787665366\n",
      "0.0767590709241835\n",
      "0.06431586377173801\n",
      "0.01924737351638612\n",
      "-0.07695497392378084\n",
      "-0.1299832974259568\n",
      "-0.27608362943444875\n",
      "-0.11700843153107801\n",
      "-0.16247904309039463\n",
      "-0.0771826080789622\n",
      "-0.03329760090075074\n",
      "-0.016771004609188244\n",
      "0.06577533390309094\n",
      "0.06361799408135461\n",
      "0.0886192657368158\n",
      "0.1394550493320574\n",
      "0.17246661920161815\n",
      "0.16409858296646646\n",
      "0.166789393852598\n",
      "0.10792332080083225\n",
      "0.03130269799415454\n",
      "0.11315986110548366\n",
      "0.20655326492698914\n",
      "0.17997412953179698\n",
      "0.18288428542182736\n",
      "0.17812682711015398\n",
      "0.2780840793195744\n",
      "0.1342364084257318\n",
      "0.14072284425141146\n",
      "0.06207186041902764\n",
      "0.06302053268590116\n",
      "0.13748253046464315\n",
      "0.06226823889419621\n",
      "0.07965227975824195\n",
      "0.09533731992156338\n",
      "0.010649579987215418\n",
      "0.037802035115850056\n",
      "0.060806845238549734\n",
      "0.09339700339040494\n",
      "0.16797936763577795\n",
      "0.14684519067257457\n",
      "0.1575906066602668\n",
      "0.17183544823717253\n",
      "0.13756941005749615\n",
      "0.13239082170676444\n",
      "0.16476402653892067\n",
      "0.1462291364392547\n",
      "0.19231896478629493\n",
      "0.1392818736143121\n",
      "0.24695925065355645\n",
      "0.3090644108908993\n",
      "0.30622712966419574\n",
      "0.3633992916612467\n",
      "0.38557263384022644\n",
      "0.3584296260247456\n",
      "0.4404489963986816\n",
      "0.3073558395874719\n",
      "0.31260531870219344\n",
      "0.26194362447099284\n",
      "0.2668613332048197\n",
      "0.32379572777983234\n",
      "0.25814266323775803\n",
      "0.34449807361837675\n",
      "0.4098696783528391\n",
      "0.4380957771357439\n",
      "0.432310343591553\n",
      "The number of windows: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val HunderedDoublesData = sc.textFile(\"HunderedDoubles.txt\").map(x => x.toDouble)\n",
    "val slidingMeans = HunderedDoublesData.sliding(20).map(x => (x.sum/x.size))\n",
    "println (\"The means of the windows of size 20: \")\n",
    "slidingMeans.collect().foreach(println)\n",
    "println (\"The number of windows: \")\n",
    "slidingMeans.collect().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) The file “multiple-sites.tsv” contains two columns: site and dwell-time. Using Spark compute the average dwell time for each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val reader = spark.read\n",
    "\n",
    "val manualSchema = new StructType(Array(\n",
    "new StructField(\"site\", IntegerType, false),\n",
    "new StructField(\"dwell-time\", IntegerType, false)\n",
    "))\n",
    "\n",
    "val multipleSites = reader.format(\"csv\").option(\"header\",true).\n",
    "option(\"delimiter\", \"\\t\").schema(manualSchema).load(\"multiple-sites.tsv\")\n",
    "\n",
    "val groupedMultipleSites = multipleSites.groupBy(\"site\")\n",
    "groupedMultipleSites.mean(\"dwell-time\").orderBy((\"site\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "7) The file “multiple-sites.tsv” contains two columns: date and dwell-time. Using Spark compute the following:\n",
    "1. The average dwell time each hour\n",
    "2. The average dwell time per day of week\n",
    "3. The average dwell time on week-days (Monday - Friday)\n",
    "4. Average dwell time on the weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------+                                             \n",
      "|Hour|Average Dwell Time each Hour|\n",
      "+----+----------------------------+\n",
      "|   0|             94.708670095518|\n",
      "|   1|           92.20954287620954|\n",
      "|   2|           96.85937970490816|\n",
      "|   3|           92.57110924839341|\n",
      "|   4|           91.33086825527253|\n",
      "|   5|           96.14621040723982|\n",
      "|   6|           92.17173727608757|\n",
      "|   7|           94.68557487038731|\n",
      "|   8|           92.35551839464883|\n",
      "|   9|           90.15113156885309|\n",
      "|  10|           92.52882433045846|\n",
      "|  11|           92.37820848611838|\n",
      "|  12|           91.21507890122736|\n",
      "|  13|           92.82834185536888|\n",
      "|  14|           95.47408666100254|\n",
      "|  15|           90.56982193064667|\n",
      "|  16|            93.1575817641229|\n",
      "|  17|           91.66231155778894|\n",
      "|  18|           93.13833634719711|\n",
      "|  19|            88.4258114374034|\n",
      "|  20|           95.00888324873097|\n",
      "|  21|           91.80379403794038|\n",
      "|  22|           97.38431752178121|\n",
      "|  23|           93.45957962813257|\n",
      "+----+----------------------------+\n",
      "\n",
      "+---------+--------------------------------------+\n",
      "|      Day|Average Dwell Time per day of the week|\n",
      "+---------+--------------------------------------+\n",
      "|Wednesday|                     91.57283771155643|\n",
      "|  Tuesday|                     89.73922367574522|\n",
      "|   Friday|                     90.44266729389628|\n",
      "| Thursday|                      91.4947995556902|\n",
      "| Saturday|                    116.88253012048193|\n",
      "|   Monday|                     89.11023872679046|\n",
      "|   Sunday|                    106.49005681818181|\n",
      "+---------+--------------------------------------+\n",
      "\n",
      "+------------------------------+\n",
      "|Average Dwell Time on Weekdays|\n",
      "+------------------------------+\n",
      "|              90.4719533927357|\n",
      "+------------------------------+\n",
      "\n",
      "+-----------------------------+                                                 \n",
      "|Average Dwell Time on Weekend|\n",
      "+-----------------------------+\n",
      "|           111.68629346933187|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val reader = spark.read\n",
    "\n",
    "val multipleDateDwellTimes = reader.format(\"csv\").option(\"header\",true).option(\"inferSchema\",true).\n",
    "option(\"sep\", \"\\t\").load(\"dwell-times.tsv\")\n",
    "\n",
    "val multipleDateSepearted = multipleDateDwellTimes.withColumnRenamed(\"date\", \"TimeStamp\").\n",
    "withColumn(\"Date\", to_date(col(\"TimeStamp\"))).\n",
    "withColumn(\"Hour\", hour(col(\"TimeStamp\"))).\n",
    "withColumn(\"Month\", date_format(col(\"Date\"),\"MMMM\")).\n",
    "withColumn(\"Day\",from_unixtime(unix_timestamp(col(\"Date\")), \"EEEEE\"))\n",
    "\n",
    "\n",
    "val averageDwellTimePerHour = multipleDateSepearted.groupBy(\"Hour\")\n",
    "val averageDwellTimePerHourValue = averageDwellTimePerHour.mean(\"dwell-time\").orderBy(\"Hour\").\n",
    "withColumnRenamed(\"avg(dwell-time)\",\"Average Dwell Time each Hour\")\n",
    "averageDwellTimePerHourValue.show(25)\n",
    "\n",
    "val averageDwellPerDay = multipleDateSepearted.groupBy(\"Day\")\n",
    "val weekAverageDays = averageDwellPerDay.mean(\"dwell-time\").\n",
    "withColumnRenamed(\"avg(dwell-time)\",\"Average Dwell Time per day of the week\")\n",
    "weekAverageDays.show()\n",
    "\n",
    "val weekDaysAverage = weekAverageDays.\n",
    "filter(not(weekAverageDays(\"Day\") === \"Sunday\")).filter(not(weekAverageDays(\"Day\") === \"Saturday\")).\n",
    "select(mean(\"Average Dwell Time per day of the week\")).\n",
    "withColumnRenamed(\"avg(Average Dwell Time per day of the week)\",\"Average Dwell Time on Weekdays\")\n",
    "weekDaysAverage.show()\n",
    "\n",
    "val weekendsAverage = weekAverageDays.\n",
    "filter(weekAverageDays(\"Day\") === \"Sunday\" || weekAverageDays(\"Day\") === \"Saturday\").\n",
    "select(mean(\"Average Dwell Time per day of the week\")).\n",
    "withColumnRenamed(\"avg(Average Dwell Time per day of the week)\",\"Average Dwell Time on Weekend\")\n",
    "weekendsAverage.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Do the average dwell times computed in #7 indicate any difference in users behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- The user spend the maximum amount of time on the website at the 22nd hour of the day (dwell-time: 97.38431752178121) and the least amount of time on the website in the 19th hour of the day (dwell-time:  88.4258114374034)\n",
    "\n",
    "- It can also be inferred that the users spends less time on the website in the weekdays(90.4719533927357) and more time on it on the weekends (111.68629346933187|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
